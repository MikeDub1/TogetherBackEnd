{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dutch-knock",
   "metadata": {},
   "source": [
    "# Task 18: Analyzing and Preprocessing the Personality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "composite-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjusted-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "internal-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(Path(\"../../SubredditData\") / \"mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adolescent-working",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...\n",
       "...    ...                                                ...\n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n",
       "8671  ENFP  'So...if this thread already exists someplace ...\n",
       "8672  INTP  'So many questions when i do these things.  I ...\n",
       "8673  INFP  'I am very conflicted right now when it comes ...\n",
       "8674  INFP  'It has been too long since I have been on per...\n",
       "\n",
       "[8675 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinated-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stopwords = stopwords.words('english') + ['INFJ', 'INFP', 'INTP', 'INTJ', 'ISFJ', 'ISFP', 'ISTP', 'ISTJ', 'ENFJ',\n",
    "                                               'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
    "                                               'people', 'People', 'think', 'Think', 'know', 'Know', \"I'm\",\n",
    "                                               'like', 'would', 'Would', 'really', 'Really', 'get', 'Get']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiwarriors/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['enfj', 'enfp', 'entj', 'entp', 'esfj', 'esfp', 'estj', 'estp', 'infj', 'infp', 'intj', 'intp', 'isfj', 'isfp', 'istj', 'istp'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=full_stopwords)\n",
    "tfidf= TfidfVectorizer(stop_words=full_stopwords)\n",
    "\n",
    "\n",
    "v_tf = vectorizer.fit_transform(df.posts)\n",
    "tf_tf = tfidf.fit_transform(df.posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-monday",
   "metadata": {},
   "source": [
    "Now, let's check the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "static-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-pathology",
   "metadata": {},
   "source": [
    "Now to make a list for every personality type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "parental-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infj_posts = df[ df['type'] == 'INFJ' ]['posts']\n",
    "\n",
    "df_intp_posts = df[df['type'] == 'INTP']['posts']\n",
    "df_intj_posts = df[df['type'] == 'INTJ']['posts']\n",
    "df_isfp_posts = df[df['type'] == 'ISFP']['posts']\n",
    "df_isfj_posts = df[df['type'] == 'ISFJ']['posts']\n",
    "df_istp_posts = df[df['type'] == 'ISTP']['posts']\n",
    "df_istj_posts = df[df['type'] == 'ISTJ']['posts']\n",
    "df_enfp_posts = df[df['type'] == 'ENFP']['posts']\n",
    "df_enfj_posts = df[df['type'] == 'ENFJ']['posts']\n",
    "df_entp_posts = df[df['type'] == 'ENTP']['posts']\n",
    "df_entj_posts = df[df['type'] == 'ENTJ']['posts']\n",
    "df_esfj_posts = df[df['type'] == 'ESFJ']['posts']\n",
    "df_esfp_posts = df[df['type'] == 'ESFP']['posts']\n",
    "df_estj_posts = df[df['type'] == 'ESTJ']['posts']\n",
    "df_estp_posts = df[df['type'] == 'ESTP']['posts']\n",
    "df_infp_posts = df[df['type'] == 'INFP']['posts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-electric",
   "metadata": {},
   "source": [
    "For every personality type, we must put all of the posts in one single array and then convert that array into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noticed-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFJ\n",
    "infj_posts = [] \n",
    "infj_text = \"\"\n",
    "for post_cluster in df_infj_posts:\n",
    "    infj_posts += post_cluster.split('|||')\n",
    "for text in infj_posts:\n",
    "    infj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#INTP\n",
    "intp_posts = []\n",
    "intp_text = \"\"\n",
    "for post_cluster in df_intp_posts:\n",
    "    intp_posts += post_cluster.split('|||')\n",
    "for text in intp_posts:\n",
    "    intp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#INTJ\n",
    "intj_posts = []\n",
    "intj_text = \"\"\n",
    "for post_cluster in df_intj_posts:\n",
    "    intj_posts += post_cluster.split('|||')\n",
    "for text in intj_posts:\n",
    "    intj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISFP\n",
    "isfp_posts = []\n",
    "isfp_text = \"\"\n",
    "for post_cluster in df_isfp_posts:\n",
    "    isfp_posts += post_cluster.split('|||')\n",
    "for text in isfp_posts:\n",
    "    isfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ISFJ\n",
    "isfj_posts = []\n",
    "isfj_text = \"\"\n",
    "for post_cluster in df_isfj_posts:\n",
    "    isfj_posts += post_cluster.split('|||')\n",
    "for text in isfj_posts:\n",
    "    isfj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISTP\n",
    "istp_posts = []\n",
    "istp_text = \"\"\n",
    "for post_cluster in df_istp_posts:\n",
    "    istp_posts += post_cluster.split('|||')\n",
    "for text in istp_posts:\n",
    "    istp_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISTJ\n",
    "istj_posts = []\n",
    "istj_text = \"\"\n",
    "for post_cluster in df_istj_posts:\n",
    "    istj_posts += post_cluster.split('|||')\n",
    "for text in istj_posts:\n",
    "    istj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ENFP\n",
    "enfp_posts = []\n",
    "enfp_text = \"\"\n",
    "for post_cluster in df_enfp_posts:\n",
    "    enfp_posts += post_cluster.split('|||')\n",
    "for text in enfp_posts:\n",
    "    enfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENFJ\n",
    "enfj_posts = []\n",
    "enfj_text = \"\"\n",
    "for post_cluster in df_enfj_posts:\n",
    "    enfj_posts += post_cluster.split('|||')\n",
    "for text in enfj_posts:\n",
    "    enfj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENTP\n",
    "entp_posts = []\n",
    "entp_text = \"\"\n",
    "for post_cluster in df_entp_posts:\n",
    "    entp_posts += post_cluster.split('|||')\n",
    "for text in intp_posts:\n",
    "    entp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENTJ\n",
    "entj_posts = []\n",
    "entj_text = \"\"\n",
    "for post_cluster in df_entj_posts:\n",
    "    entj_posts += post_cluster.split('|||')\n",
    "for text in entj_posts:\n",
    "    entj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ESFJ\n",
    "esfj_posts = []\n",
    "esfj_text = \"\"\n",
    "for post_cluster in df_esfj_posts:\n",
    "    esfj_posts += post_cluster.split('|||')\n",
    "for text in esfj_posts:\n",
    "    esfj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ESFP\n",
    "esfp_posts = []\n",
    "esfp_text = \"\"\n",
    "for post_cluster in df_esfp_posts:\n",
    "    esfp_posts += post_cluster.split('|||')\n",
    "for text in esfp_posts:\n",
    "    esfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "\n",
    "#ESTJ\n",
    "estj_posts = []\n",
    "estj_text = \"\"\n",
    "for post_cluster in df_estj_posts:\n",
    "    estj_posts += post_cluster.split('|||')\n",
    "for text in estj_posts:\n",
    "    estj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ESTP\n",
    "estp_posts = []\n",
    "estp_text = \"\"\n",
    "for post_cluster in df_estp_posts:\n",
    "    estp_posts += post_cluster.split('|||')\n",
    "for text in estp_posts:\n",
    "    estp_text += \"\".join(text)+\" \"\n",
    "\n",
    "#INFP\n",
    "infp_posts = []\n",
    "infp_text = \"\"\n",
    "for post_cluster in df_infp_posts:\n",
    "    infp_posts += post_cluster.split('|||')\n",
    "for text in infp_posts:\n",
    "    infp_text += \"\".join(text)+\" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-trial",
   "metadata": {},
   "source": [
    "Before we generate the wordclouds, let's preprocess this text a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sudden-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedditScrape import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "skilled-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "infj_text = preprocess(infj_text)\n",
    "intp_test = preprocess(intp_text)\n",
    "intj_text = preprocess(intj_text)\n",
    "isfp_text = preprocess(isfp_text)\n",
    "isfj_text = preprocess(isfj_text)\n",
    "istp_text = preprocess(istp_text)\n",
    "istj_text = preprocess(istj_text)\n",
    "infp_text = preprocess(infp_text)\n",
    "enfj_text = preprocess(enfj_text)\n",
    "entp_text = preprocess(entp_text)\n",
    "entj_text = preprocess(entj_text)\n",
    "esfp_text = preprocess(esfp_text)\n",
    "esfj_text = preprocess(esfj_text)\n",
    "estp_text = preprocess(estp_text)\n",
    "estj_text = preprocess(estj_text)\n",
    "enfp_text = preprocess(enfp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-christmas",
   "metadata": {},
   "source": [
    "Now time to make the wordclouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "alert-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_infj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(infj_text)\n",
    "wc_intp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(intp_text)\n",
    "wc_intj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(intj_text)\n",
    "wc_isfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(isfp_text)\n",
    "wc_isfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(isfj_text)\n",
    "wc_istp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(istp_text)\n",
    "wc_istj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(istj_text)\n",
    "wc_infp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(infp_text)\n",
    "wc_enfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(enfj_text)\n",
    "wc_enfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(enfp_text)\n",
    "wc_entj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(entj_text)\n",
    "wc_entp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(entp_text)\n",
    "wc_esfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(esfj_text)\n",
    "wc_esfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(esfp_text)\n",
    "wc_estp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(estp_text)\n",
    "wc_estj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(estj_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: SAVE ALL AS PNG!!!!!\n",
    "\n",
    "wc_infj.to_file('infj.png')\n",
    "wc_intp.to_file('intp.png')\n",
    "wc_intj.to_file('intj.png')\n",
    "wc_isfp.to_file('isfp.png')\n",
    "wc_istp.to_file('istp.png')\n",
    "wc_istj.to_file('istj.png')\n",
    "wc_infp.to_file('infp.png')\n",
    "wc_enfj.to_file('enfj.png')\n",
    "wc_enfp.to_file('enfp.png')\n",
    "wc_entj.to_file('entj.png')\n",
    "wc_esfj.to_file('esfj.png')\n",
    "wc_esfp.to_file('esfp.png')\n",
    "wc_estj.to_file('estj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-partition",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_entp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_enfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_enfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_entj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_infj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_infp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_intp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_intj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_isfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_isfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_istj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_istp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_esfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_estj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_esfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-horse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ESTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_estp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-juvenile",
   "metadata": {},
   "source": [
    "## Preparing the dataset for the next task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-disposal",
   "metadata": {},
   "source": [
    "Let's try to balance out the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "described-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infj = df[df['type'] == 'INFJ']\n",
    "df_intp = df[df['type'] == 'INTP']\n",
    "df_intj = df[df['type'] == 'INTJ']\n",
    "df_isfp = df[df['type'] == 'ISFP']\n",
    "df_isfj = df[df['type'] == 'ISFJ']\n",
    "df_istp = df[df['type'] == 'ISTP']\n",
    "df_istj = df[df['type'] == 'ISTJ']\n",
    "df_enfp = df[df['type'] == 'ENFP']\n",
    "df_enfj = df[df['type'] == 'ENFJ']\n",
    "df_entp = df[df['type'] == 'ENTP']\n",
    "df_entj = df[df['type'] == 'ENTJ']\n",
    "df_esfj = df[df['type'] == 'ESFJ']\n",
    "df_esfp = df[df['type'] == 'ESFP']\n",
    "df_estj = df[df['type'] == 'ESTJ']\n",
    "df_estp = df[df['type'] == 'ESTP']\n",
    "df_infp = df[df['type'] == 'INFP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "secret-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_infj_r = resample(df_infj, replace=True, n_samples=300, random_state=1234)\n",
    "df_infp_r = resample(df_infp, replace=True, n_samples=300, random_state=1234)\n",
    "df_intj_r = resample(df_intj, replace=True, n_samples=300, random_state=1234)\n",
    "df_intp_r = resample(df_intp, replace=True, n_samples=300, random_state=1234)\n",
    "df_isfp_r = resample(df_isfp, replace=True, n_samples=300, random_state=1234)\n",
    "df_isfj_r = resample(df_isfj, replace=True, n_samples=300, random_state=1234)\n",
    "df_istp_r = resample(df_istp, replace=True, n_samples=300, random_state=1234)\n",
    "df_istj_r = resample(df_istj, replace=True, n_samples=300, random_state=1234)\n",
    "\n",
    "df_enfj_r = resample(df_enfj, replace=True, n_samples=300, random_state=1234)\n",
    "df_enfp_r = resample(df_enfp, replace=True, n_samples=300, random_state=1234)\n",
    "df_entj_r = resample(df_entj, replace=True, n_samples=300, random_state=1234)\n",
    "df_entp_r = resample(df_entp, replace=True, n_samples=300, random_state=1234)\n",
    "df_esfp_r = resample(df_esfp, replace=True, n_samples=300, random_state=1234)\n",
    "df_esfj_r = resample(df_esfj, replace=True, n_samples=300, random_state=1234)\n",
    "df_estp_r = resample(df_estp, replace=True, n_samples=300, random_state=1234)\n",
    "df_estj_r = resample(df_estj, replace=True, n_samples=300, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compliant-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish\n",
    "df_resampled = pd.concat([df_infj_r, df_infp_r, df_intj_r, df_intp_r, df_isfp_r, df_isfj_r, df_intp_r, df_istj_r, df_enfj_r,df_enfp_r, df_entj_r, df_entp_r, df_esfp_r, df_esfj_r, df_estp_r, df_estj_r])\n",
    "\n",
    "#Convert this dataframe into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lesser-scout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Hahaha! I was watching it too! What did you f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Yes. I can carry conversations. Just this und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Half of it is going straight to charity, anot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I'm good! Our house was unscathed :D   Yeah, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7246</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'It depends on the group but in the past, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'Mausi, Girl, that was awesome reading. It rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'Let me try to answer your points one by one. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>intj|||isfp|||They have been taught to live a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6034</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'Fellow artist,  I apologize for failing to ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'People have their priorities. It sounds like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "4676  INFJ  'Hahaha! I was watching it too! What did you f...\n",
       "4190  INFJ  'Yes. I can carry conversations. Just this und...\n",
       "7807  INFJ  'Half of it is going straight to charity, anot...\n",
       "6326  INFJ  'I'm good! Our house was unscathed :D   Yeah, ...\n",
       "7246  INFJ  'It depends on the group but in the past, with...\n",
       "...    ...                                                ...\n",
       "1916  ESTJ  'Mausi, Girl, that was awesome reading. It rea...\n",
       "2344  ESTJ  'Let me try to answer your points one by one. ...\n",
       "595   ESTJ  intj|||isfp|||They have been taught to live a ...\n",
       "6034  ESTJ  'Fellow artist,  I apologize for failing to ac...\n",
       "1450  ESTJ  'People have their priorities. It sounds like ...\n",
       "\n",
       "[4800 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-specialist",
   "metadata": {},
   "source": [
    "300 * 16 = 4800 and there are 4800, so the resampling was successful! Now it is time to append the top 5 most used words from every personality type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beautiful-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "infj_top_words = list(wc_infj.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "appointed-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "infp_top_words = list(wc_infp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aboriginal-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "intj_top_words = list(wc_intj.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "uniform-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "intp_top_words = list(wc_intp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ranking-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfp_top_words = list(wc_isfp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "growing-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfj_top_words = list(wc_isfj.words_.keys())[1:20]\n",
    "istp_top_words = list(wc_istp.words_.keys())[1:20]\n",
    "istj_top_words = list(wc_istj.words_.keys())[1:20]\n",
    "enfp_top_words = list(wc_enfp.words_.keys())[1:20]\n",
    "enfj_top_words = list(wc_enfj.words_.keys())[1:20]\n",
    "entp_top_words = list(wc_entp.words_.keys())[1:20]\n",
    "entj_top_words = list(wc_entj.words_.keys())[1:20]\n",
    "esfp_top_words = list(wc_esfp.words_.keys())[1:20]\n",
    "esfj_top_words = list(wc_esfj.words_.keys())[1:20]\n",
    "estj_top_words = list(wc_estj.words_.keys())[1:20]\n",
    "estp_top_words = list(wc_estp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faced-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_top_words = list(set(infj_top_words + infp_top_words + intj_top_words + intp_top_words + isfp_top_words + isfj_top_words + istj_top_words + enfp_top_words + enfj_top_words + entp_top_words + entj_top_words + esfp_top_words + esfj_top_words + estj_top_words + estp_top_words).difference(['www youtube', 'youtube com', 'com watchv', 'www youtub', 'youtub com']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "direct-fancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watch v',\n",
       " 'something',\n",
       " 'friend',\n",
       " 'way',\n",
       " 'need',\n",
       " 'say',\n",
       " 'thank',\n",
       " 'im',\n",
       " 'never',\n",
       " 'ive',\n",
       " 'good',\n",
       " 'well',\n",
       " 'want',\n",
       " 'com watch',\n",
       " 'person',\n",
       " 'feel',\n",
       " 'love',\n",
       " 'thing',\n",
       " 'someth',\n",
       " 'see',\n",
       " 'go',\n",
       " 'also',\n",
       " 'dont',\n",
       " 'peopl',\n",
       " 'time',\n",
       " 'realli',\n",
       " 'work',\n",
       " 'use',\n",
       " 'make',\n",
       " 'lot',\n",
       " 'much',\n",
       " 'tri',\n",
       " 'type']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "genetic-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>time</th>\n",
       "      <th>com watch</th>\n",
       "      <th>lot</th>\n",
       "      <th>say</th>\n",
       "      <th>much</th>\n",
       "      <th>im</th>\n",
       "      <th>want</th>\n",
       "      <th>friend</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>never</th>\n",
       "      <th>see</th>\n",
       "      <th>peopl</th>\n",
       "      <th>thing</th>\n",
       "      <th>go</th>\n",
       "      <th>someth</th>\n",
       "      <th>thank</th>\n",
       "      <th>need</th>\n",
       "      <th>tri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'Hahaha! I was watching it too! What did you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'Yes. I can carry conversations. Just this und...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'Half of it is going straight to charity, anot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'I'm good! Our house was unscathed :D   Yeah, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'It depends on the group but in the past, with...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'Mausi, Girl, that was awesome reading. It rea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'Let me try to answer your points one by one. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>NaN</td>\n",
       "      <td>intj|||isfp|||They have been taught to live a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'Fellow artist,  I apologize for failing to ac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>NaN</td>\n",
       "      <td>'People have their priorities. It sounds like ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts  time  \\\n",
       "0      NaN  'Hahaha! I was watching it too! What did you f...   NaN   \n",
       "1      NaN  'Yes. I can carry conversations. Just this und...   NaN   \n",
       "2      NaN  'Half of it is going straight to charity, anot...   NaN   \n",
       "3      NaN  'I'm good! Our house was unscathed :D   Yeah, ...   NaN   \n",
       "4      NaN  'It depends on the group but in the past, with...   NaN   \n",
       "...    ...                                                ...   ...   \n",
       "4795   NaN  'Mausi, Girl, that was awesome reading. It rea...   NaN   \n",
       "4796   NaN  'Let me try to answer your points one by one. ...   NaN   \n",
       "4797   NaN  intj|||isfp|||They have been taught to live a ...   NaN   \n",
       "4798   NaN  'Fellow artist,  I apologize for failing to ac...   NaN   \n",
       "4799   NaN  'People have their priorities. It sounds like ...   NaN   \n",
       "\n",
       "      com watch  lot  say  much  im  want  friend  ...  work  never  see  \\\n",
       "0           NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "1           NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "2           NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "3           NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "4           NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "...         ...  ...  ...   ...  ..   ...     ...  ...   ...    ...  ...   \n",
       "4795        NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "4796        NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "4797        NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "4798        NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "4799        NaN  NaN  NaN   NaN NaN   NaN     NaN  ...   NaN    NaN  NaN   \n",
       "\n",
       "      peopl  thing  go  someth  thank  need  tri  \n",
       "0       NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "1       NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "2       NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "3       NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "4       NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "...     ...    ...  ..     ...    ...   ...  ...  \n",
       "4795    NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "4796    NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "4797    NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "4798    NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "4799    NaN    NaN NaN     NaN    NaN   NaN  NaN  \n",
       "\n",
       "[4800 rows x 34 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resampled = pd.read_csv(\"DfResampled.csv\")\n",
    "df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "anticipated-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...|||84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...|||Welcome and stuff.|||http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.|||Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|||Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...|||All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...|||Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:|||https://www.youtube.com/watch?v=QyPqT8umzmY|||It appears to be too late. :sad:|||There's someone out there for everyone.|||Wait... I thought confidence was a good thing.|||I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|||Yo entp ladies... if you're into a complimentary personality,well, hey.|||... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.|||http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50|||http://www.youtube.com/watch?v=msqXffgh7b8|||Banned because this thread requires it of me.|||Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.|||http://www.youtube.com/watch?v=Mw7eoU3BMbE|||http://www.youtube.com/watch?v=4V2uYORhQOk|||http://www.youtube.com/watch?v=SlVmgFQQ0TI|||Banned for too many b's in that sentence. How could you! Think of the B!|||Banned for watching movies in the corner with the dunces.|||Banned because Health class clearly taught you nothing about peer pressure.|||Banned for a whole host of reasons!|||http://www.youtube.com/watch?v=IRcrv41hgz4|||1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...|||a pokemon world  an infj society  everyone becomes an optimist|||49142|||http://www.youtube.com/watch?v=ZRCEq_JFeFM|||http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg|||http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif|||http://www.serebii.net/potw-dp/Scizor.jpg|||Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.|||Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:|||Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.|||http://www.youtube.com/watch?v=w8IgImn57aQ|||Banned for being too much of a thundering, grumbling kind of storm... yep.|||Ahh... old high school music I haven't heard in ages.   http://www.youtube.com/watch?v=dcCRUPCdB1w|||I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...|||I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M|||Move to the Denver area and start a new life for myself.'\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = df['posts'][0]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elegant-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled.to_csv('DfFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
