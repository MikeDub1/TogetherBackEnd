{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "catholic-document",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import gzip\n",
    "import nltk\n",
    "import re\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.optimizers import SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "royal-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename = \"embeddings_word2vec.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', filename), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "compact-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'type': 'personalities'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "mysterious-chart",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117871 unique tokens\n",
      "Shape of the posts tensor (8000, 1938)\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "mbti = []\n",
    "\n",
    "for post in range(len(df.posts)):\n",
    "    sentences.append(df['posts'].iloc[post])\n",
    "    mbti.append(df['personalities'].iloc[post])\n",
    "\n",
    "max_length = max([len(sentence.split()) for sentence in sentences])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "print(\"Shape of the posts tensor\", sequences_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pregnant-ozone",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3b76b2206447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_index' is not defined"
     ]
    }
   ],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 150))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words: continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "willing-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def prepare_labels(y_train, y_test):\n",
    "    OHE = OneHotEncoder()\n",
    "    OHE.fit(y_train)\n",
    "    y_train_enc = OHE.transform(y_train)\n",
    "    y_test_enc = OHE.transform(y_test)\n",
    "    return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "optional-aviation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-33d482b4f498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#preprocessing these motherfuckers with scikit-learn...\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dataset = df.values\n",
    "\n",
    "X = dataset[:,-1]\n",
    "Y = dataset[:,0]\n",
    "\n",
    "\n",
    "#cv = CountVectorizer(stop_words='english')\n",
    "all_features = cv.fit_transform(X)\n",
    "\n",
    "X_train_sk, X_test_sk, y_train_sk, y_test_sk = train_test_split(all_features, Y, test_size=0.33, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "#X_train_sk, X_test_sk = prepare_inputs(X_train_sk, X_test_sk)\n",
    "y_train_sk, y_test_sk = prepare_labels(y_train_sk, y_test_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "signed-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = df.values\n",
    "Y = dataset[:,0]\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "\n",
    "indicies = np.arange(sequences_padded.shape[0])\n",
    "np.random.shuffle(indicies)\n",
    "sequences_padded =sequences_padded[indicies]\n",
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * sequences_padded.shape[0])\n",
    "X_train_pad = sequences_padded[:-num_validation_samples]\n",
    "X_test_pad = sequences_padded[-num_validation_samples:]\n",
    "\n",
    "y_train = Y[:-num_validation_samples]\n",
    "y_test = Y[-num_validation_samples:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "excellent-namibia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad (5600, 1938)\n",
      "Shape of y_train (5600, 1)\n",
      "Shape of X_test_pad (2400, 1938)\n",
      "Shape of y_test (2400, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['INTJ'],\n",
       "       ['ESFP'],\n",
       "       ['INFJ'],\n",
       "       ...,\n",
       "       ['ENFJ'],\n",
       "       ['ESFJ'],\n",
       "       ['ENTJ']], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of X_train_pad\", X_train_pad.shape)\n",
    "print(\"Shape of y_train\", y_train.shape)\n",
    "print(\"Shape of X_test_pad\", X_test_pad.shape)\n",
    "print(\"Shape of y_test\", y_test.shape)\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "single-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_train, y_test = prepare_labels(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "accessory-treasury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5600x15 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5600 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "exclusive-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "num_words = len(word_index) + 1\n",
    "inputs = tf.keras.Input(shape=(max_length,))\n",
    "\n",
    "embedding_layer = Embedding(input_dim=num_words,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=max_length)(inputs)\n",
    "\n",
    "gru = tf.keras.layers.Bidirectional(GRU(units=256, return_sequences=True))(embedding_layer)\n",
    "\n",
    "\n",
    "flatten = Flatten()(gru)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "#lr_schedule = ExponentialDecay(initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.9)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss= \"categorical_crossentropy\", optimizer='adam', metrics=['accuracy',\n",
    "                                                                     tf.keras.metrics.AUC(name='auc')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fitted-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING ANOTHER MODEL USING SEQUENTIAL JIC\n",
    "embedding_dim=150\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, input_length=max_length))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=SGD(lr=0.01), metrics=['accuracy',\n",
    "                                                                     tf.keras.metrics.AUC(name='auc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "endangered-consolidation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(min_lr=1e-5, max_lr=1e-1, steps_per_epoch=np.ceil(125), epochs=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "boring-diesel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 534s - loss: 0.0000e+00 - accuracy: 0.0586 - auc: 0.4911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1ca863e710>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train, batch_size=64, callbacks=[lr_finder], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "important-opera",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad, \n",
    "          y_train, \n",
    "          batch_size=64, \n",
    "          epochs=25,\n",
    "          validation_data=(X_test_pad, y_test),\n",
    "          callbacks=[tf.keras.callbacks.ModelCheckpoint('./personality_model.h5', save_best_only=True, save_weights_only=True)],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model tests here after you continue working...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "right-publication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 211s 17s/step - loss: 0.0000e+00 - accuracy: 0.0611 - auc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test_pad, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "requested-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRFinder by jeremyjordan on Github:\n",
    "\n",
    "# LRFinder by jeremyjordan on Github:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    \n",
    "    '''\n",
    "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
    "    \n",
    "    # Usage\n",
    "        ```python\n",
    "            lr_finder = LRFinder(min_lr=1e-5, \n",
    "                                 max_lr=1e-2, \n",
    "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
    "                                 epochs=3)\n",
    "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
    "            \n",
    "            lr_finder.plot_loss()\n",
    "        ```\n",
    "    \n",
    "    # Arguments\n",
    "        min_lr: The lower bound of the learning rate range for the experiment.\n",
    "        max_lr: The upper bound of the learning rate range for the experiment.\n",
    "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
    "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
    "        \n",
    "    # References\n",
    "        Blog post: jeremyjordan.me/nn-learning-rate\n",
    "        Original paper: https://arxiv.org/abs/1506.01186\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_iterations = steps_per_epoch * epochs\n",
    "        self.iteration = 0\n",
    "        self.history = {}\n",
    "        \n",
    "    def clr(self):\n",
    "        '''Calculate the learning rate.'''\n",
    "        x = self.iteration / self.total_iterations \n",
    "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
    "        logs = logs or {}\n",
    "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
    "        \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        '''Record previous batch statistics and update the learning rate.'''\n",
    "        logs = logs or {}\n",
    "        self.iteration += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.iteration)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    " \n",
    "    def plot_lr(self):\n",
    "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
    "        plt.plot(self.history['iterations'], self.history['lr'])\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning rate')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "featured-breakfast",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = iFuckedUp_savingDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opened-bridge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/aiwarriors/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "    \n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "    \n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def preprocess(posts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    new_posts = cleanHtml(posts)\n",
    "    new_posts = cleanPunc(new_posts)\n",
    "    new_posts = keepAlpha(new_posts)\n",
    "   \n",
    "    \n",
    "    return_posts = \" \".join(lemmatizer.lemmatize(word, pos=\"v\") for word in new_posts.split())\n",
    "    return return_posts\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "connected-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iFuckedUp_savingDf():\n",
    "    df = pd.read_csv('DfFinal.csv')\n",
    "    df_original_posts = pd.read_csv(\"mbti_1.csv\")['posts']\n",
    "    df_original_posts = pd.DataFrame(df_original_posts)\n",
    "    df = df.drop(['posts'], axis=1)\n",
    "    df = df.join(df_original_posts)\n",
    "    \n",
    "    for i in range(len(df.index)):\n",
    "        df.at[i, 'posts'] = preprocess(\" \".join(df['posts'].iloc[i].split('|||')))\n",
    "    df.to_csv('DfFinal.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-bottom",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>Oooi Jully Beans Bem vinda ao PerC Ok n o sei ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3799</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>g w Its good for her then cause I be a beast w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>814</td>\n",
       "      <td>ESFJ</td>\n",
       "      <td>Have yall give people the impression you where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2280</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>Here go Excuse my lowkey high look be sick all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2851</td>\n",
       "      <td>ENFJ</td>\n",
       "      <td>the maltese falcon and the usual suspect i fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>3988</td>\n",
       "      <td>INFP</td>\n",
       "      <td>http www youtube com watchv jfTNcewDKS Im not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>8658</td>\n",
       "      <td>ESFJ</td>\n",
       "      <td>Yoda We havent be cook long enough put us back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>2352</td>\n",
       "      <td>INFP</td>\n",
       "      <td>This isnt even an I feel like statement anymor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>54</td>\n",
       "      <td>INFP</td>\n",
       "      <td>Noted that I have be note Also its you shock I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>7380</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>Fiend and yet I notice its absence from this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  type                                              posts\n",
       "0             24  ENTJ  Oooi Jully Beans Bem vinda ao PerC Ok n o sei ...\n",
       "1           3799  INFJ  g w Its good for her then cause I be a beast w...\n",
       "2            814  ESFJ  Have yall give people the impression you where...\n",
       "3           2280  INFJ  Here go Excuse my lowkey high look be sick all...\n",
       "4           2851  ENFJ  the maltese falcon and the usual suspect i fin...\n",
       "...          ...   ...                                                ...\n",
       "4795        3988  INFP  http www youtube com watchv jfTNcewDKS Im not ...\n",
       "4796        8658  ESFJ  Yoda We havent be cook long enough put us back...\n",
       "4797        2352  INFP  This isnt even an I feel like statement anymor...\n",
       "4798          54  INFP  Noted that I have be note Also its you shock I...\n",
       "4799        7380  ENFP  Fiend and yet I notice its absence from this m...\n",
       "\n",
       "[4800 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "southern-money",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['level_0' 'index'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c04e273d670c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['level_0' 'index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['level_0', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupational-holocaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personalities</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESFP</td>\n",
       "      <td>I loathe BuzzFeed https www youtube com watchv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESTP</td>\n",
       "      <td>Yes I get this all the time At least some of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>I think I should have read The Dubliners earli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESFJ</td>\n",
       "      <td>Whoa ive be go for close to months and the pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INTP</td>\n",
       "      <td>Sorry to hear about what youre go through That...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>ISFJ</td>\n",
       "      <td>Yes its a lot like a chess game Ive be in a si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>ESFP</td>\n",
       "      <td>http www youtube com watchv epRB P vvE Cute EN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>INFP</td>\n",
       "      <td>When someone immediately dismiss anything I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>https www youtube com watchv LozrF EHf Q Youve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>Yes hyperbolize the reactions of empathetic pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     personalities                                              posts\n",
       "0             ESFP  I loathe BuzzFeed https www youtube com watchv...\n",
       "1             ESTP  Yes I get this all the time At least some of m...\n",
       "2             ENFP  I think I should have read The Dubliners earli...\n",
       "3             ESFJ  Whoa ive be go for close to months and the pla...\n",
       "4             INTP  Sorry to hear about what youre go through That...\n",
       "...            ...                                                ...\n",
       "7995          ISFJ  Yes its a lot like a chess game Ive be in a si...\n",
       "7996          ESFP  http www youtube com watchv epRB P vvE Cute EN...\n",
       "7997          INFP  When someone immediately dismiss anything I ha...\n",
       "7998          ISFP  https www youtube com watchv LozrF EHf Q Youve...\n",
       "7999          ENFP  Yes hyperbolize the reactions of empathetic pe...\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-affairs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-switch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(df.posts).toarray()\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X.reindex(np.random.permutation(X.index))\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, df.values[:,0], test_size=0.33, random_state=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nbclf=gbc()\n",
    "parameters = {\n",
    "    #\"loss\":[\"deviance\"],\n",
    "     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "     \"n_estimators\":[10,15,20,30,50,75,100]\n",
    "    }\n",
    "clf = GridSearchCV(nbclf, parameters,scoring='accuracy',refit=False,cv=2, n_jobs=-1)\n",
    "clf.fit(X_tr,y_tr)\n",
    "\n",
    "clf.bestparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-invalid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-algebra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(texts_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-pricing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
