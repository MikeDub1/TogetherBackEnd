{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dutch-knock",
   "metadata": {},
   "source": [
    "# Task 18: Analyzing and Preprocessing the Personality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "composite-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjusted-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "internal-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(Path(\"../../SubredditData\") / \"mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adolescent-working",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'So...if this thread already exists someplace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "0     INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1     ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2     INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3     INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4     ENTJ  'You're fired.|||That's another silly misconce...\n",
       "...    ...                                                ...\n",
       "8670  ISFP  'https://www.youtube.com/watch?v=t8edHB_h908||...\n",
       "8671  ENFP  'So...if this thread already exists someplace ...\n",
       "8672  INTP  'So many questions when i do these things.  I ...\n",
       "8673  INFP  'I am very conflicted right now when it comes ...\n",
       "8674  INFP  'It has been too long since I have been on per...\n",
       "\n",
       "[8675 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinated-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stopwords = stopwords.words('english') + ['INFJ', 'INFP', 'INTP', 'INTJ', 'ISFJ', 'ISFP', 'ISTP', 'ISTJ', 'ENFJ',\n",
    "                                               'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
    "                                               'people', 'People', 'think', 'Think', 'know', 'Know', \"I'm\",\n",
    "                                               'like', 'would', 'Would', 'really', 'Really', 'get', 'Get']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiwarriors/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['enfj', 'enfp', 'entj', 'entp', 'esfj', 'esfp', 'estj', 'estp', 'infj', 'infp', 'intj', 'intp', 'isfj', 'isfp', 'istj', 'istp'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=full_stopwords)\n",
    "tfidf= TfidfVectorizer(stop_words=full_stopwords)\n",
    "\n",
    "\n",
    "v_tf = vectorizer.fit_transform(df.posts)\n",
    "tf_tf = tfidf.fit_transform(df.posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-monday",
   "metadata": {},
   "source": [
    "Now, let's check the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "static-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-pathology",
   "metadata": {},
   "source": [
    "Now to make a list for every personality type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parental-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infj_posts = df[ df['type'] == 'INFJ' ]['posts']\n",
    "\n",
    "df_intp_posts = df[df['type'] == 'INTP']['posts']\n",
    "df_intj_posts = df[df['type'] == 'INTJ']['posts']\n",
    "df_isfp_posts = df[df['type'] == 'ISFP']['posts']\n",
    "df_isfj_posts = df[df['type'] == 'ISFJ']['posts']\n",
    "df_istp_posts = df[df['type'] == 'ISTP']['posts']\n",
    "df_istj_posts = df[df['type'] == 'ISTJ']['posts']\n",
    "df_enfp_posts = df[df['type'] == 'ENFP']['posts']\n",
    "df_enfj_posts = df[df['type'] == 'ENFJ']['posts']\n",
    "df_entp_posts = df[df['type'] == 'ENTP']['posts']\n",
    "df_entj_posts = df[df['type'] == 'ENTJ']['posts']\n",
    "df_esfj_posts = df[df['type'] == 'ESFJ']['posts']\n",
    "df_esfp_posts = df[df['type'] == 'ESFP']['posts']\n",
    "df_estj_posts = df[df['type'] == 'ESTJ']['posts']\n",
    "df_estp_posts = df[df['type'] == 'ESTP']['posts']\n",
    "df_infp_posts = df[df['type'] == 'INFP']['posts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-electric",
   "metadata": {},
   "source": [
    "For every personality type, we must put all of the posts in one single array and then convert that array into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noticed-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFJ\n",
    "infj_posts = [] \n",
    "infj_text = \"\"\n",
    "for post_cluster in df_infj_posts:\n",
    "    infj_posts += post_cluster.split('|||')\n",
    "for text in infj_posts:\n",
    "    infj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#INTP\n",
    "intp_posts = []\n",
    "intp_text = \"\"\n",
    "for post_cluster in df_intp_posts:\n",
    "    intp_posts += post_cluster.split('|||')\n",
    "for text in intp_posts:\n",
    "    intp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#INTJ\n",
    "intj_posts = []\n",
    "intj_text = \"\"\n",
    "for post_cluster in df_intj_posts:\n",
    "    intj_posts += post_cluster.split('|||')\n",
    "for text in intj_posts:\n",
    "    intj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISFP\n",
    "isfp_posts = []\n",
    "isfp_text = \"\"\n",
    "for post_cluster in df_isfp_posts:\n",
    "    isfp_posts += post_cluster.split('|||')\n",
    "for text in isfp_posts:\n",
    "    isfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ISFJ\n",
    "isfj_posts = []\n",
    "isfj_text = \"\"\n",
    "for post_cluster in df_isfj_posts:\n",
    "    isfj_posts += post_cluster.split('|||')\n",
    "for text in isfj_posts:\n",
    "    isfj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISTP\n",
    "istp_posts = []\n",
    "istp_text = \"\"\n",
    "for post_cluster in df_istp_posts:\n",
    "    istp_posts += post_cluster.split('|||')\n",
    "for text in istp_posts:\n",
    "    istp_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ISTJ\n",
    "istj_posts = []\n",
    "istj_text = \"\"\n",
    "for post_cluster in df_istj_posts:\n",
    "    istj_posts += post_cluster.split('|||')\n",
    "for text in istj_posts:\n",
    "    istj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ENFP\n",
    "enfp_posts = []\n",
    "enfp_text = \"\"\n",
    "for post_cluster in df_enfp_posts:\n",
    "    enfp_posts += post_cluster.split('|||')\n",
    "for text in enfp_posts:\n",
    "    enfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENFJ\n",
    "enfj_posts = []\n",
    "enfj_text = \"\"\n",
    "for post_cluster in df_enfj_posts:\n",
    "    enfj_posts += post_cluster.split('|||')\n",
    "for text in enfj_posts:\n",
    "    enfj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENTP\n",
    "entp_posts = []\n",
    "entp_text = \"\"\n",
    "for post_cluster in df_entp_posts:\n",
    "    entp_posts += post_cluster.split('|||')\n",
    "for text in intp_posts:\n",
    "    entp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ENTJ\n",
    "entj_posts = []\n",
    "entj_text = \"\"\n",
    "for post_cluster in df_entj_posts:\n",
    "    entj_posts += post_cluster.split('|||')\n",
    "for text in entj_posts:\n",
    "    entj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ESFJ\n",
    "esfj_posts = []\n",
    "esfj_text = \"\"\n",
    "for post_cluster in df_esfj_posts:\n",
    "    esfj_posts += post_cluster.split('|||')\n",
    "for text in esfj_posts:\n",
    "    esfj_text += \"\".join(text)+\" \"\n",
    "\n",
    "#ESFP\n",
    "esfp_posts = []\n",
    "esfp_text = \"\"\n",
    "for post_cluster in df_esfp_posts:\n",
    "    esfp_posts += post_cluster.split('|||')\n",
    "for text in esfp_posts:\n",
    "    esfp_text += \"\".join(text)+\" \"\n",
    "    \n",
    "\n",
    "#ESTJ\n",
    "estj_posts = []\n",
    "estj_text = \"\"\n",
    "for post_cluster in df_estj_posts:\n",
    "    estj_posts += post_cluster.split('|||')\n",
    "for text in estj_posts:\n",
    "    estj_text += \"\".join(text)+\" \"\n",
    "    \n",
    "#ESTP\n",
    "estp_posts = []\n",
    "estp_text = \"\"\n",
    "for post_cluster in df_estp_posts:\n",
    "    estp_posts += post_cluster.split('|||')\n",
    "for text in estp_posts:\n",
    "    estp_text += \"\".join(text)+\" \"\n",
    "\n",
    "#INFP\n",
    "infp_posts = []\n",
    "infp_text = \"\"\n",
    "for post_cluster in df_infp_posts:\n",
    "    infp_posts += post_cluster.split('|||')\n",
    "for text in infp_posts:\n",
    "    infp_text += \"\".join(text)+\" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-trial",
   "metadata": {},
   "source": [
    "Before we generate the wordclouds, let's preprocess this text a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sudden-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RedditScrape import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "skilled-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "infj_text = preprocess(infj_text)\n",
    "intp_test = preprocess(intp_text)\n",
    "intj_text = preprocess(intj_text)\n",
    "isfp_text = preprocess(isfp_text)\n",
    "isfj_text = preprocess(isfj_text)\n",
    "istp_text = preprocess(istp_text)\n",
    "istj_text = preprocess(istj_text)\n",
    "infp_text = preprocess(infp_text)\n",
    "enfj_text = preprocess(enfj_text)\n",
    "entp_text = preprocess(entp_text)\n",
    "entj_text = preprocess(entj_text)\n",
    "esfp_text = preprocess(esfp_text)\n",
    "esfj_text = preprocess(esfj_text)\n",
    "estp_text = preprocess(estp_text)\n",
    "estj_text = preprocess(estj_text)\n",
    "enfp_text = preprocess(enfp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-christmas",
   "metadata": {},
   "source": [
    "Now time to make the wordclouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "alert-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_infj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(infj_text)\n",
    "wc_intp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(intp_text)\n",
    "wc_intj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(intj_text)\n",
    "wc_isfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(isfp_text)\n",
    "wc_isfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(isfj_text)\n",
    "wc_istp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(istp_text)\n",
    "wc_istj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(istj_text)\n",
    "wc_infp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(infp_text)\n",
    "wc_enfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(enfj_text)\n",
    "wc_enfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(enfp_text)\n",
    "wc_entj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(entj_text)\n",
    "wc_entp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(entp_text)\n",
    "wc_esfj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(esfj_text)\n",
    "wc_esfp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(esfp_text)\n",
    "wc_estp = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(estp_text)\n",
    "wc_estj = WordCloud(width=800, height=800, stopwords = set(full_stopwords), min_font_size=12).generate(estj_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: SAVE ALL AS PNG!!!!!\n",
    "\n",
    "wc_infj.to_file('infj.png')\n",
    "wc_intp.to_file('intp.png')\n",
    "wc_intj.to_file('intj.png')\n",
    "wc_isfp.to_file('isfp.png')\n",
    "wc_istp.to_file('istp.png')\n",
    "wc_istj.to_file('istj.png')\n",
    "wc_infp.to_file('infp.png')\n",
    "wc_enfj.to_file('enfj.png')\n",
    "wc_enfp.to_file('enfp.png')\n",
    "wc_entj.to_file('entj.png')\n",
    "wc_esfj.to_file('esfj.png')\n",
    "wc_esfp.to_file('esfp.png')\n",
    "wc_estj.to_file('estj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-partition",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_entp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_enfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_enfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_entj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_infj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_infp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_intp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_intj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_isfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_isfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_istj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_istp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESFJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_esfj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTJ\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_estj) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESFP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_esfp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-horse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ESTP\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wc_estp) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-juvenile",
   "metadata": {},
   "source": [
    "## Preparing the dataset for the next task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-disposal",
   "metadata": {},
   "source": [
    "Let's try to balance out the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "described-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infj = df[df['type'] == 'INFJ']\n",
    "df_intp = df[df['type'] == 'INTP']\n",
    "df_intj = df[df['type'] == 'INTJ']\n",
    "df_isfp = df[df['type'] == 'ISFP']\n",
    "df_isfj = df[df['type'] == 'ISFJ']\n",
    "df_istp = df[df['type'] == 'ISTP']\n",
    "df_istj = df[df['type'] == 'ISTJ']\n",
    "df_enfp = df[df['type'] == 'ENFP']\n",
    "df_enfj = df[df['type'] == 'ENFJ']\n",
    "df_entp = df[df['type'] == 'ENTP']\n",
    "df_entj = df[df['type'] == 'ENTJ']\n",
    "df_esfj = df[df['type'] == 'ESFJ']\n",
    "df_esfp = df[df['type'] == 'ESFP']\n",
    "df_estj = df[df['type'] == 'ESTJ']\n",
    "df_estp = df[df['type'] == 'ESTP']\n",
    "df_infp = df[df['type'] == 'INFP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "secret-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_infj_r = resample(df_infj, replace=True, n_samples=300, random_state=1234)\n",
    "df_infp_r = resample(df_infp, replace=True, n_samples=300, random_state=1234)\n",
    "df_intj_r = resample(df_intj, replace=True, n_samples=300, random_state=1234)\n",
    "df_intp_r = resample(df_intp, replace=True, n_samples=300, random_state=1234)\n",
    "df_isfp_r = resample(df_isfp, replace=True, n_samples=300, random_state=1234)\n",
    "df_isfj_r = resample(df_isfj, replace=True, n_samples=300, random_state=1234)\n",
    "df_istp_r = resample(df_istp, replace=True, n_samples=300, random_state=1234)\n",
    "df_istj_r = resample(df_istj, replace=True, n_samples=300, random_state=1234)\n",
    "\n",
    "df_enfj_r = resample(df_enfj, replace=True, n_samples=300, random_state=1234)\n",
    "df_enfp_r = resample(df_enfp, replace=True, n_samples=300, random_state=1234)\n",
    "df_entj_r = resample(df_entj, replace=True, n_samples=300, random_state=1234)\n",
    "df_entp_r = resample(df_entp, replace=True, n_samples=300, random_state=1234)\n",
    "df_esfp_r = resample(df_esfp, replace=True, n_samples=300, random_state=1234)\n",
    "df_esfj_r = resample(df_esfj, replace=True, n_samples=300, random_state=1234)\n",
    "df_estp_r = resample(df_estp, replace=True, n_samples=300, random_state=1234)\n",
    "df_estj_r = resample(df_estj, replace=True, n_samples=300, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "compliant-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish\n",
    "df_resampled = pd.concat([df_infj_r, df_infp_r, df_intj_r, df_intp_r, df_isfp_r, df_isfj_r, df_intp_r, df_istj_r, df_enfj_r,df_enfp_r, df_entj_r, df_entp_r, df_esfp_r, df_esfj_r, df_estp_r, df_estj_r])\n",
    "\n",
    "#Convert this dataframe into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lesser-scout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Hahaha! I was watching it too! What did you f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4190</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Yes. I can carry conversations. Just this und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7807</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Half of it is going straight to charity, anot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I'm good! Our house was unscathed :D   Yeah, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7246</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'It depends on the group but in the past, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'Mausi, Girl, that was awesome reading. It rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'Let me try to answer your points one by one. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>intj|||isfp|||They have been taught to live a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6034</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'Fellow artist,  I apologize for failing to ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>'People have their priorities. It sounds like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                              posts\n",
       "4676  INFJ  'Hahaha! I was watching it too! What did you f...\n",
       "4190  INFJ  'Yes. I can carry conversations. Just this und...\n",
       "7807  INFJ  'Half of it is going straight to charity, anot...\n",
       "6326  INFJ  'I'm good! Our house was unscathed :D   Yeah, ...\n",
       "7246  INFJ  'It depends on the group but in the past, with...\n",
       "...    ...                                                ...\n",
       "1916  ESTJ  'Mausi, Girl, that was awesome reading. It rea...\n",
       "2344  ESTJ  'Let me try to answer your points one by one. ...\n",
       "595   ESTJ  intj|||isfp|||They have been taught to live a ...\n",
       "6034  ESTJ  'Fellow artist,  I apologize for failing to ac...\n",
       "1450  ESTJ  'People have their priorities. It sounds like ...\n",
       "\n",
       "[4800 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-specialist",
   "metadata": {},
   "source": [
    "300 * 16 = 4800 and there are 4800, so the resampling was successful! Now it is time to append the top 5 most used words from every personality type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "beautiful-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "infj_top_words = list(wc_infj.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "appointed-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "infp_top_words = list(wc_infp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aboriginal-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "intj_top_words = list(wc_intj.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "uniform-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "intp_top_words = list(wc_intp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ranking-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfp_top_words = list(wc_isfp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "growing-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "isfj_top_words = list(wc_isfj.words_.keys())[1:20]\n",
    "istp_top_words = list(wc_istp.words_.keys())[1:20]\n",
    "istj_top_words = list(wc_istj.words_.keys())[1:20]\n",
    "enfp_top_words = list(wc_enfp.words_.keys())[1:20]\n",
    "enfj_top_words = list(wc_enfj.words_.keys())[1:20]\n",
    "entp_top_words = list(wc_entp.words_.keys())[1:20]\n",
    "entj_top_words = list(wc_entj.words_.keys())[1:20]\n",
    "esfp_top_words = list(wc_esfp.words_.keys())[1:20]\n",
    "esfj_top_words = list(wc_esfj.words_.keys())[1:20]\n",
    "estj_top_words = list(wc_estj.words_.keys())[1:20]\n",
    "estp_top_words = list(wc_estp.words_.keys())[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "faced-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_top_words = list(set(infj_top_words + infp_top_words + intj_top_words + intp_top_words + isfp_top_words + isfj_top_words + istj_top_words + enfp_top_words + enfj_top_words + entp_top_words + entj_top_words + esfp_top_words + esfj_top_words + estj_top_words + estp_top_words).difference(['www youtube', 'youtube com', 'com watchv', 'www youtub', 'youtub com']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "direct-fancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'way',\n",
       " 'friend',\n",
       " 'much',\n",
       " 'go',\n",
       " 'realli',\n",
       " 'ive',\n",
       " 'type',\n",
       " 'feel',\n",
       " 'well',\n",
       " 'something',\n",
       " 'com watch',\n",
       " 'never',\n",
       " 'also',\n",
       " 'thank',\n",
       " 'lot',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'peopl',\n",
       " 'tri',\n",
       " 'use',\n",
       " 'see',\n",
       " 'work',\n",
       " 'someth',\n",
       " 'watch v',\n",
       " 'need',\n",
       " 'im',\n",
       " 'person',\n",
       " 'good',\n",
       " 'make',\n",
       " 'say',\n",
       " 'thing',\n",
       " 'love']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
